# torchmc

Implementing [ChEES-HMC](http://proceedings.mlr.press/v130/hoffman21a/hoffman21a.pdf) in PyTorch.

Because:

> NUTSâ€™s relatively complex control flow makes it difficult to efficiently run many parallel chains on accelerators such as GPUs.

> Implementing parallel-chain HMC on top of a vector-oriented library such as TensorFlow, JAX, or PyTorch that supports automatic differentiation is relatively easy as long as the chains run in lockstep; a batch of gradients for the chains can be computed in parallel quickly on SIMD accelerators such as GPUs or TPUs (Lao et al., 2020). As we will see in section 3, a GPU can run tens of chains in parallel nearly as fast as a CPU core can run a single chain.